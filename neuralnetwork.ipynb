{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neuralnetwork.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fraziermatthew/ImageAI/blob/master/neuralnetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "tQDQZJMatmph",
        "colab_type": "code",
        "outputId": "1a9a719d-d3f6-446c-d058-9929c52df2e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ugnQ4g4Itt1O",
        "colab_type": "code",
        "outputId": "df4f8623-d5fe-4fdc-f062-64c1b5689faf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 922
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install tensorflow-gpu\n",
        "!pip3 install numpy\n",
        "!pip3 install scipy\n",
        "!pip3 install opencv-python\n",
        "!pip3 install pillow\n",
        "!pip3 install matplotlib\n",
        "!pip3 install h5py\n",
        "!pip3 install keras"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/7e/bec4d62e9dc95e828922c6cec38acd9461af8abe749f7c9def25ec4b2fdb/tensorflow_gpu-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (281.7MB)\n",
            "\u001b[K    100% |████████████████████████████████| 281.7MB 84kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.6)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.7)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.6.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.9)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow-gpu) (3.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow-gpu) (0.14.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu) (40.8.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.12.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.14.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.14.6)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (3.4.5.20)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.14.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow) (0.46)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.14.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.3.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (40.8.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.11.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.11.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xjl8HphWugQU",
        "colab_type": "code",
        "outputId": "e6a7fdc9-470a-4ae3-8fc2-d42d9b2b7dd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd /usr/local/src"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NmWHJ_jdt6Vr",
        "colab_type": "code",
        "outputId": "ade3a5d1-28ee-4a5b-d607-ceb12894d432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "cell_type": "code",
      "source": [
        "# Import the dependencies\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "# Load the mnist dataset\n",
        "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "train_x = train_x.astype('float32') / 255\n",
        "test_x = test_x.astype('float32') / 255\n",
        "\n",
        "# Print to verify the setup - shapes of the data arrays\n",
        "print(\"Train Images: \",train_x.shape)\n",
        "print(\"Train Labels: \",train_y.shape)\n",
        "print(\"Test Images: \",test_x.shape)\n",
        "print(\"Test Labels: \",test_y.shape)\n",
        "\n",
        "# Flatten the image pixels\n",
        "train_x = train_x.reshape(60000,784)\n",
        "test_x = test_x.reshape(10000,784)\n",
        "\n",
        "# Convert labels to vectors\n",
        "train_y = keras.utils.to_categorical(train_y,10)\n",
        "test_y = keras.utils.to_categorical(test_y,10)\n",
        "\n",
        "# Define your model/network\n",
        "model = Sequential()\n",
        "model.add(Dense(units=128,activation=\"relu\",input_shape=(784,)))\n",
        "model.add(Dense(units=128,activation=\"relu\"))\n",
        "model.add(Dense(units=128,activation=\"relu\"))\n",
        "model.add(Dense(units=10,activation=\"softmax\"))\n",
        "\n",
        "# Compile the function\n",
        "model.compile(optimizer=SGD(0.013),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "# Uncomment both lines during training\n",
        "# Fit the function\n",
        "# model.fit(train_x,train_y,batch_size=32,epochs=20,shuffle=True,verbose=1)\n",
        "\n",
        "# Save the model\n",
        "# model.save(\"mnistmodel.h5\")\n",
        "\n",
        "# Comment out this line during training\n",
        "model.load_weights(\"mnistmodel.h5\")\n",
        "\n",
        "# Evaluate Accuracy\n",
        "accuracy = model.evaluate(x=test_x,y=test_y,batch_size=32)\n",
        "print(\"Accuracy: \",accuracy[1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Images:  (60000, 28, 28)\n",
            "Train Labels:  (60000,)\n",
            "Test Images:  (10000, 28, 28)\n",
            "Test Labels:  (10000,)\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 7s 124us/step - loss: 0.5644 - acc: 0.8457\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.2434 - acc: 0.9288\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.1836 - acc: 0.9460\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1484 - acc: 0.9561\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1255 - acc: 0.9632\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.1081 - acc: 0.9683\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0947 - acc: 0.9721\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0835 - acc: 0.9762\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0746 - acc: 0.9788\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0668 - acc: 0.9806\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0612 - acc: 0.9825\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0553 - acc: 0.9838\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0501 - acc: 0.9857\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0453 - acc: 0.9869\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0410 - acc: 0.9884\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0375 - acc: 0.9892\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0344 - acc: 0.9902\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0310 - acc: 0.9915\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 7s 121us/step - loss: 0.0282 - acc: 0.9921\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 7s 120us/step - loss: 0.0256 - acc: 0.9930\n",
            "10000/10000 [==============================] - 1s 55us/step\n",
            "Accuracy:  0.9764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NefHIAOn0lGz",
        "colab_type": "code",
        "outputId": "39556fae-0a43-4ee5-e894-82e4446097f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "cell_type": "code",
      "source": [
        "# import needed classes\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense,Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the mnist dataset\n",
        "(train_x, train_y) , (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=128,activation=\"relu\",input_shape=(784,)))\n",
        "model.add(Dense(units=128,activation=\"relu\"))\n",
        "model.add(Dense(units=128,activation=\"relu\"))\n",
        "model.add(Dense(units=10,activation=\"softmax\"))\n",
        "\n",
        "# Specify the training components\n",
        "model.compile(optimizer=SGD(0.01),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "# Load the pretrained model\n",
        "model.load_weights(\"mnistmodel.h5\")\n",
        "\n",
        "# Normalize the test dataset\n",
        "test_x = test_x.astype('float32') / 255\n",
        "\n",
        "# Extract a specific image\n",
        "img = test_x[167]\n",
        "\n",
        "# Create a flattened copy of the image\n",
        "test_img = img.reshape((1,784))\n",
        "\n",
        "# Predict the class\n",
        "img_class = model.predict_classes(test_img)\n",
        "classname = img_class[0]\n",
        "print(\"Class: \",classname)\n",
        "\n",
        "# Display the original non-flattened copy of the image\n",
        "plt.title(\"Prediction Result: %s\"%(classname))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class:  5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFZCAYAAAARqQ0OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICwBwOB/QCWWJgttYkUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "z1SDYGq11snZ",
        "colab_type": "code",
        "outputId": "d46af9e9-1574-48ac-fc50-f74acf5142d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!curl -O https://raw.githubusercontent.com/johnolafenwa/Neural-Network-Tutorials/master/basics/testimage.png"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2918  100  2918    0     0  16770      0 --:--:-- --:--:-- --:--:-- 16770\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FXX6EIUP2ROI",
        "colab_type": "code",
        "outputId": "10c8c3b4-7a01-4437-ac5c-13c04c303882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "cell_type": "code",
      "source": [
        "# Import needed classes\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=128,activation=\"relu\",input_shape=(784,)))\n",
        "model.add(Dense(units=128,activation=\"relu\"))\n",
        "model.add(Dense(units=128,activation=\"relu\"))\n",
        "model.add(Dense(units=10,activation=\"softmax\"))\n",
        "\n",
        "# Specify the training components\n",
        "model.compile(optimizer=SGD(0.01),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "# Load the pretrained model\n",
        "model.load_weights(\"mnistmodel.h5\")\n",
        "\n",
        "# Load an image from your system\n",
        "img = image.load_img(path=\"testimage.png\",grayscale=True,target_size=(28,28))\n",
        "img = image.img_to_array(img)\n",
        "img = img.reshape((28,28))\n",
        "\n",
        "# Create a flattened copy of the image\n",
        "test_img = img.reshape((1,784))\n",
        "\n",
        "# Predict the class\n",
        "img_class = model.predict_classes(test_img)\n",
        "classname = img_class[0]\n",
        "print(\"Class: \",classname)\n",
        "\n",
        "# Display the original non-flattened copy of the image\n",
        "plt.title(\"Prediction Result: %s\"%(classname))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py:98: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
            "  warnings.warn('grayscale is deprecated. Please use '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Class:  3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFZCAYAAAARqQ0OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "wORq6hN12yEt",
        "colab_type": "code",
        "outputId": "287aa525-7ad3-43a5-87e5-9d3e886a15d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1176
        }
      },
      "cell_type": "code",
      "source": [
        "# Import needed classes\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# Load the mnist dataset\n",
        "(train_x, train_y) , (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "train_x = train_x.astype('float32') / 255\n",
        "test_x = test_x.astype('float32') / 255\n",
        "\n",
        "# Print the shapes of the data arrays\n",
        "print(\"Train Images: \",train_x.shape)\n",
        "print(\"Train Labels: \",train_y.shape)\n",
        "print(\"Test Images: \",test_x.shape)\n",
        "print(\"Test Labels: \",test_y.shape)\n",
        "\n",
        "# Flatten the images\n",
        "train_x = train_x.reshape(60000,784)\n",
        "test_x = test_x.reshape(10000,784)\n",
        "\n",
        "# Encode the labels to vectors\n",
        "train_y = keras.utils.to_categorical(train_y,10)\n",
        "test_y = keras.utils.to_categorical(test_y,10)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=128,activation=\"relu\",input_shape=(784,)))\n",
        "model.add(Dense(units=128,activation=\"relu\"))\n",
        "model.add(Dense(units=128,activation=\"relu\"))\n",
        "model.add(Dense(units=10,activation=\"softmax\"))\n",
        "\n",
        "# Define the Learning rate schedule function\n",
        "def lr_schedule(epoch):\n",
        "  lr = 0.1\n",
        "  if epoch > 15:\n",
        "    lr = lr / 100\n",
        "  elif epoch > 10:\n",
        "    lr = lr / 10\n",
        "  elif epoch > 5:\n",
        "    lr = lr / 5\n",
        "  print(\"Learning Rate: \",lr)\n",
        "  return lr\n",
        "\n",
        "# Pass the scheduler function to the Learning Rate Scheduler class\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# Specify the training components\n",
        "model.compile(optimizer=SGD(lr_schedule(0)),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "#Fit the model\n",
        "model.fit(train_x,train_y,batch_size=32,epochs=20,shuffle=True,verbose=1,callbacks=[lr_scheduler])\n",
        "\n",
        "#Evaluate the accuracy of the test dataset\n",
        "accuracy = model.evaluate(x=test_x,y=test_y,batch_size=32)\n",
        "print(\"Accuracy: \",accuracy[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Images:  (60000, 28, 28)\n",
            "Train Labels:  (60000,)\n",
            "Test Images:  (10000, 28, 28)\n",
            "Test Labels:  (10000,)\n",
            "Learning Rate:  0.1\n",
            "Epoch 1/20\n",
            "Learning Rate:  0.1\n",
            "60000/60000 [==============================] - 8s 128us/step - loss: 0.2797 - acc: 0.9140\n",
            "Epoch 2/20\n",
            "Learning Rate:  0.1\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.1124 - acc: 0.9655\n",
            "Epoch 3/20\n",
            "Learning Rate:  0.1\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.0798 - acc: 0.9746\n",
            "Epoch 4/20\n",
            "Learning Rate:  0.1\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0598 - acc: 0.9809\n",
            "Epoch 5/20\n",
            "Learning Rate:  0.1\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.0475 - acc: 0.9847\n",
            "Epoch 6/20\n",
            "Learning Rate:  0.1\n",
            "60000/60000 [==============================] - 7s 124us/step - loss: 0.0375 - acc: 0.9878\n",
            "Epoch 7/20\n",
            "Learning Rate:  0.02\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0159 - acc: 0.9957\n",
            "Epoch 8/20\n",
            "Learning Rate:  0.02\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0110 - acc: 0.9975\n",
            "Epoch 9/20\n",
            "Learning Rate:  0.02\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.0093 - acc: 0.9981\n",
            "Epoch 10/20\n",
            "Learning Rate:  0.02\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.0080 - acc: 0.9985\n",
            "Epoch 11/20\n",
            "Learning Rate:  0.02\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0071 - acc: 0.9987\n",
            "Epoch 12/20\n",
            "Learning Rate:  0.01\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0058 - acc: 0.9991\n",
            "Epoch 13/20\n",
            "Learning Rate:  0.01\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.0054 - acc: 0.9992\n",
            "Epoch 14/20\n",
            "Learning Rate:  0.01\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0051 - acc: 0.9993\n",
            "Epoch 15/20\n",
            "Learning Rate:  0.01\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0049 - acc: 0.9994\n",
            "Epoch 16/20\n",
            "Learning Rate:  0.01\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0046 - acc: 0.9994\n",
            "Epoch 17/20\n",
            "Learning Rate:  0.001\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0041 - acc: 0.9996\n",
            "Epoch 18/20\n",
            "Learning Rate:  0.001\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0041 - acc: 0.9996\n",
            "Epoch 19/20\n",
            "Learning Rate:  0.001\n",
            "60000/60000 [==============================] - 7s 122us/step - loss: 0.0041 - acc: 0.9996\n",
            "Epoch 20/20\n",
            "Learning Rate:  0.001\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.0041 - acc: 0.9996\n",
            "10000/10000 [==============================] - 1s 64us/step\n",
            "Accuracy:  0.9839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JeCvwGwa35VG",
        "colab_type": "code",
        "outputId": "98c85b13-bd19-447d-a950-f178592fac91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2128
        }
      },
      "cell_type": "code",
      "source": [
        "# Import needed classes\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "\n",
        "# Load the mnist dataset\n",
        "(train_x, train_y) , (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "train_x = train_x.astype('float32') / 255\n",
        "test_x = test_x.astype('float32') / 255\n",
        "\n",
        "# Print the shapes of the data arrays\n",
        "print(\"Train Images: \",train_x.shape)\n",
        "print(\"Train Labels: \",train_y.shape)\n",
        "print(\"Test Images: \",test_x.shape)\n",
        "print(\"Test Labels: \",test_y.shape)\n",
        "\n",
        "# Flatten the images\n",
        "train_x = train_x.reshape(60000,784)\n",
        "test_x = test_x.reshape(10000,784)\n",
        "\n",
        "# Encode the labels to vectors\n",
        "train_y = keras.utils.to_categorical(train_y,10)\n",
        "test_y = keras.utils.to_categorical(test_y,10)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=128,activation=\"relu\",input_shape=(784,)))\n",
        "model.add(Dense(units=128,activation=\"relu\"))\n",
        "model.add(Dense(units=128,activation=\"relu\"))\n",
        "model.add(Dense(units=10,activation=\"softmax\"))\n",
        "\n",
        "# Print a Summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Define the Learning rate schedule function\n",
        "def lr_schedule(epoch):\n",
        "  lr = 0.1\n",
        "  if epoch > 15:\n",
        "    lr = lr / 100\n",
        "  elif epoch > 10:\n",
        "    lr = lr / 10\n",
        "  elif epoch > 5:\n",
        "    lr = lr / 5\n",
        "  print(\"Learning Rate: \",lr)\n",
        "  return lr\n",
        "\n",
        "# Pass the scheduler function to the Learning Rate Scheduler class\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# Directory in which to create models\n",
        "save_direc = os.path.join(os.getcwd(), 'mnistsavedmodels')\n",
        "\n",
        "# Name of model files\n",
        "model_name = 'mnistmodel.{epoch:03d}.h5'\n",
        "\n",
        "# Create Directory if it doesn't exist\n",
        "if not os.path.isdir(save_direc):\n",
        "  os.makedirs(save_direc)\n",
        "\n",
        "# Join the directory with the model file\n",
        "modelpath = os.path.join(save_direc, model_name)\n",
        "checkpoint = ModelCheckpoint(filepath=modelpath,\n",
        " monitor='val_acc',\n",
        " verbose=1,\n",
        " period=1)\n",
        "\n",
        "# Specify the training components\n",
        "model.compile(optimizer=SGD(lr_schedule(0)),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "# model.fit(train_x,train_y,batch_size=32,epochs=20,shuffle=True,verbose=1,callbacks=[checkpoint,lr_scheduler])\n",
        "# model.fit(train_x,train_y,batch_size=32,epochs=20,shuffle=True,validation_split=0.1,verbose=1,callbacks=[checkpoint,lr_scheduler])\n",
        "model.fit(train_x,train_y,batch_size=32,epochs=20,shuffle=True,validation_data=[test_x,test_y],verbose=1,callbacks=[checkpoint,lr_scheduler])\n",
        "\n",
        "# Evaluate the accuracy of the test dataset\n",
        "accuracy = model.evaluate(x=test_x,y=test_y,batch_size=32)\n",
        "print(\"Accuracy: \",accuracy[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Images:  (60000, 28, 28)\n",
            "Train Labels:  (60000,)\n",
            "Test Images:  (10000, 28, 28)\n",
            "Test Labels:  (10000,)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 134,794\n",
            "Trainable params: 134,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Learning Rate:  0.1\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "Learning Rate:  0.1\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.2743 - acc: 0.9157 - val_loss: 0.1665 - val_acc: 0.9487\n",
            "\n",
            "Epoch 00001: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.001.h5\n",
            "Epoch 2/20\n",
            "Learning Rate:  0.1\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.1113 - acc: 0.9656 - val_loss: 0.0970 - val_acc: 0.9699\n",
            "\n",
            "Epoch 00002: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.002.h5\n",
            "Epoch 3/20\n",
            "Learning Rate:  0.1\n",
            "60000/60000 [==============================] - 8s 125us/step - loss: 0.0795 - acc: 0.9752 - val_loss: 0.0908 - val_acc: 0.9709\n",
            "\n",
            "Epoch 00003: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.003.h5\n",
            "Epoch 4/20\n",
            "Learning Rate:  0.1\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0595 - acc: 0.9813 - val_loss: 0.0815 - val_acc: 0.9771\n",
            "\n",
            "Epoch 00004: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.004.h5\n",
            "Epoch 5/20\n",
            "Learning Rate:  0.1\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0488 - acc: 0.9842 - val_loss: 0.0775 - val_acc: 0.9762\n",
            "\n",
            "Epoch 00005: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.005.h5\n",
            "Epoch 6/20\n",
            "Learning Rate:  0.1\n",
            "60000/60000 [==============================] - 8s 127us/step - loss: 0.0384 - acc: 0.9875 - val_loss: 0.0792 - val_acc: 0.9788\n",
            "\n",
            "Epoch 00006: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.006.h5\n",
            "Epoch 7/20\n",
            "Learning Rate:  0.02\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0167 - acc: 0.9953 - val_loss: 0.0658 - val_acc: 0.9821\n",
            "\n",
            "Epoch 00007: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.007.h5\n",
            "Epoch 8/20\n",
            "Learning Rate:  0.02\n",
            "60000/60000 [==============================] - 7s 124us/step - loss: 0.0118 - acc: 0.9974 - val_loss: 0.0644 - val_acc: 0.9819\n",
            "\n",
            "Epoch 00008: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.008.h5\n",
            "Epoch 9/20\n",
            "Learning Rate:  0.02\n",
            "60000/60000 [==============================] - 7s 125us/step - loss: 0.0097 - acc: 0.9979 - val_loss: 0.0644 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00009: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.009.h5\n",
            "Epoch 10/20\n",
            "Learning Rate:  0.02\n",
            "60000/60000 [==============================] - 7s 125us/step - loss: 0.0085 - acc: 0.9983 - val_loss: 0.0666 - val_acc: 0.9827\n",
            "\n",
            "Epoch 00010: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.010.h5\n",
            "Epoch 11/20\n",
            "Learning Rate:  0.02\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0073 - acc: 0.9987 - val_loss: 0.0668 - val_acc: 0.9816\n",
            "\n",
            "Epoch 00011: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.011.h5\n",
            "Epoch 12/20\n",
            "Learning Rate:  0.01\n",
            "60000/60000 [==============================] - 7s 125us/step - loss: 0.0060 - acc: 0.9991 - val_loss: 0.0675 - val_acc: 0.9824\n",
            "\n",
            "Epoch 00012: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.012.h5\n",
            "Epoch 13/20\n",
            "Learning Rate:  0.01\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.0056 - acc: 0.9992 - val_loss: 0.0680 - val_acc: 0.9823\n",
            "\n",
            "Epoch 00013: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.013.h5\n",
            "Epoch 14/20\n",
            "Learning Rate:  0.01\n",
            "60000/60000 [==============================] - 8s 125us/step - loss: 0.0053 - acc: 0.9993 - val_loss: 0.0688 - val_acc: 0.9817\n",
            "\n",
            "Epoch 00014: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.014.h5\n",
            "Epoch 15/20\n",
            "Learning Rate:  0.01\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.0050 - acc: 0.9993 - val_loss: 0.0688 - val_acc: 0.9821\n",
            "\n",
            "Epoch 00015: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.015.h5\n",
            "Epoch 16/20\n",
            "Learning Rate:  0.01\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.0048 - acc: 0.9993 - val_loss: 0.0695 - val_acc: 0.9819\n",
            "\n",
            "Epoch 00016: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.016.h5\n",
            "Epoch 17/20\n",
            "Learning Rate:  0.001\n",
            "60000/60000 [==============================] - 7s 123us/step - loss: 0.0043 - acc: 0.9995 - val_loss: 0.0694 - val_acc: 0.9821\n",
            "\n",
            "Epoch 00017: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.017.h5\n",
            "Epoch 18/20\n",
            "Learning Rate:  0.001\n",
            "60000/60000 [==============================] - 7s 125us/step - loss: 0.0042 - acc: 0.9995 - val_loss: 0.0694 - val_acc: 0.9821\n",
            "\n",
            "Epoch 00018: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.018.h5\n",
            "Epoch 19/20\n",
            "Learning Rate:  0.001\n",
            "60000/60000 [==============================] - 7s 124us/step - loss: 0.0042 - acc: 0.9995 - val_loss: 0.0696 - val_acc: 0.9821\n",
            "\n",
            "Epoch 00019: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.019.h5\n",
            "Epoch 20/20\n",
            "Learning Rate:  0.001\n",
            "60000/60000 [==============================] - 7s 124us/step - loss: 0.0042 - acc: 0.9995 - val_loss: 0.0695 - val_acc: 0.9820\n",
            "\n",
            "Epoch 00020: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.020.h5\n",
            "10000/10000 [==============================] - 0s 45us/step\n",
            "Accuracy:  0.982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3MsWGJT4dxqS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2162
        },
        "outputId": "88d4d7c9-acbb-4554-b6b5-eecbd649be75"
      },
      "cell_type": "code",
      "source": [
        "# Using the functional API\n",
        "# Import needed classes\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense\n",
        "from keras.models import Model,Input\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "\n",
        "# Load the mnist dataset\n",
        "(train_x, train_y) , (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "train_x = train_x.astype('float32') / 255\n",
        "test_x = test_x.astype('float32') / 255\n",
        "\n",
        "# Print the shapes of the data arrays\n",
        "print(\"Train Images: \",train_x.shape)\n",
        "print(\"Train Labels: \",train_y.shape)\n",
        "print(\"Test Images: \",test_x.shape)\n",
        "print(\"Test Labels: \",test_y.shape)\n",
        "\n",
        "# Flatten the images\n",
        "train_x = train_x.reshape(60000,784)\n",
        "test_x = test_x.reshape(10000,784)\n",
        "\n",
        "# Encode the labels to vectors\n",
        "train_y = keras.utils.to_categorical(train_y,10)\n",
        "test_y = keras.utils.to_categorical(test_y,10)\n",
        "\n",
        "# Define the model\n",
        "def MiniModel(input_shape):\n",
        "  images = Input(input_shape)\n",
        "  net = Dense(units=128,activation=\"relu\")(images)\n",
        "  net = Dense(units=128, activation=\"relu\")(net)\n",
        "  net = Dense(units=128, activation=\"relu\")(net)\n",
        "  net = Dense(units=10,activation=\"softmax\")(net)\n",
        "  model = Model(inputs=images,outputs=net)\n",
        "  return model\n",
        "\n",
        "# Print a Summary of the model\n",
        "model = MiniModel((784,))\n",
        "model.summary()\n",
        "\n",
        "# Define the Learning rate schedule function\n",
        "def lr_schedule(epoch):\n",
        "  lr = 0.1\n",
        "  if epoch > 15:\n",
        "    lr = lr / 100\n",
        "  elif epoch > 10:\n",
        "    lr = lr / 10\n",
        "  elif epoch > 5:\n",
        "    lr = lr / 5\n",
        "  print(\"Learning Rate: \",lr)\n",
        "  return lr\n",
        "\n",
        "# Pass the scheduler function to the Learning Rate Scheduler class\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# Directory in which to create models\n",
        "save_direc = os.path.join(os.getcwd(), 'mnistsavedmodels')\n",
        "\n",
        "# Name of model files\n",
        "model_name = 'mnistmodel.{epoch:03d}.h5'\n",
        "\n",
        "# Create Directory if it doesn't exist\n",
        "if not os.path.isdir(save_direc):\n",
        "  os.makedirs(save_direc)\n",
        "\n",
        "# Join the directory with the model file\n",
        "modelpath = os.path.join(save_direc, model_name)\n",
        "checkpoint = ModelCheckpoint(filepath=modelpath,\n",
        " monitor='val_acc',\n",
        " verbose=1,\n",
        " period=1)\n",
        "\n",
        "# Specify the training components\n",
        "model.compile(optimizer=SGD(lr_schedule(0)),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(train_x,train_y,batch_size=32,epochs=20,shuffle=True,validation_split=0.1,verbose=1,callbacks=[checkpoint,lr_scheduler])\n",
        "\n",
        "# Evaluate the accuracy of the test dataset\n",
        "accuracy = model.evaluate(x=test_x,y=test_y,batch_size=32)\n",
        "print(\"Accuracy: \",accuracy[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Images:  (60000, 28, 28)\n",
            "Train Labels:  (60000,)\n",
            "Test Images:  (10000, 28, 28)\n",
            "Test Labels:  (10000,)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 134,794\n",
            "Trainable params: 134,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Learning Rate:  0.1\n",
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/20\n",
            "Learning Rate:  0.1\n",
            "54000/54000 [==============================] - 7s 128us/step - loss: 0.3024 - acc: 0.9078 - val_loss: 0.1081 - val_acc: 0.9687\n",
            "\n",
            "Epoch 00001: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.001.h5\n",
            "Epoch 2/20\n",
            "Learning Rate:  0.1\n",
            "54000/54000 [==============================] - 7s 124us/step - loss: 0.1215 - acc: 0.9629 - val_loss: 0.0846 - val_acc: 0.9755\n",
            "\n",
            "Epoch 00002: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.002.h5\n",
            "Epoch 3/20\n",
            "Learning Rate:  0.1\n",
            "54000/54000 [==============================] - 7s 124us/step - loss: 0.0841 - acc: 0.9741 - val_loss: 0.0882 - val_acc: 0.9732\n",
            "\n",
            "Epoch 00003: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.003.h5\n",
            "Epoch 4/20\n",
            "Learning Rate:  0.1\n",
            "54000/54000 [==============================] - 7s 123us/step - loss: 0.0653 - acc: 0.9790 - val_loss: 0.0785 - val_acc: 0.9773\n",
            "\n",
            "Epoch 00004: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.004.h5\n",
            "Epoch 5/20\n",
            "Learning Rate:  0.1\n",
            "54000/54000 [==============================] - 7s 122us/step - loss: 0.0508 - acc: 0.9839 - val_loss: 0.0743 - val_acc: 0.9772\n",
            "\n",
            "Epoch 00005: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.005.h5\n",
            "Epoch 6/20\n",
            "Learning Rate:  0.1\n",
            "54000/54000 [==============================] - 7s 121us/step - loss: 0.0408 - acc: 0.9870 - val_loss: 0.0887 - val_acc: 0.9740\n",
            "\n",
            "Epoch 00006: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.006.h5\n",
            "Epoch 7/20\n",
            "Learning Rate:  0.02\n",
            "54000/54000 [==============================] - 7s 123us/step - loss: 0.0168 - acc: 0.9954 - val_loss: 0.0623 - val_acc: 0.9823\n",
            "\n",
            "Epoch 00007: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.007.h5\n",
            "Epoch 8/20\n",
            "Learning Rate:  0.02\n",
            "54000/54000 [==============================] - 7s 123us/step - loss: 0.0121 - acc: 0.9972 - val_loss: 0.0615 - val_acc: 0.9823\n",
            "\n",
            "Epoch 00008: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.008.h5\n",
            "Epoch 9/20\n",
            "Learning Rate:  0.02\n",
            "54000/54000 [==============================] - 7s 122us/step - loss: 0.0102 - acc: 0.9978 - val_loss: 0.0618 - val_acc: 0.9828\n",
            "\n",
            "Epoch 00009: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.009.h5\n",
            "Epoch 10/20\n",
            "Learning Rate:  0.02\n",
            "54000/54000 [==============================] - 7s 122us/step - loss: 0.0088 - acc: 0.9982 - val_loss: 0.0627 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00010: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.010.h5\n",
            "Epoch 11/20\n",
            "Learning Rate:  0.02\n",
            "54000/54000 [==============================] - 7s 122us/step - loss: 0.0076 - acc: 0.9986 - val_loss: 0.0639 - val_acc: 0.9838\n",
            "\n",
            "Epoch 00011: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.011.h5\n",
            "Epoch 12/20\n",
            "Learning Rate:  0.01\n",
            "54000/54000 [==============================] - 7s 123us/step - loss: 0.0064 - acc: 0.9991 - val_loss: 0.0640 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00012: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.012.h5\n",
            "Epoch 13/20\n",
            "Learning Rate:  0.01\n",
            "54000/54000 [==============================] - 7s 123us/step - loss: 0.0059 - acc: 0.9991 - val_loss: 0.0645 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00013: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.013.h5\n",
            "Epoch 14/20\n",
            "Learning Rate:  0.01\n",
            "54000/54000 [==============================] - 7s 123us/step - loss: 0.0056 - acc: 0.9992 - val_loss: 0.0651 - val_acc: 0.9820\n",
            "\n",
            "Epoch 00014: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.014.h5\n",
            "Epoch 15/20\n",
            "Learning Rate:  0.01\n",
            "54000/54000 [==============================] - 7s 123us/step - loss: 0.0053 - acc: 0.9994 - val_loss: 0.0654 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00015: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.015.h5\n",
            "Epoch 16/20\n",
            "Learning Rate:  0.01\n",
            "54000/54000 [==============================] - 7s 123us/step - loss: 0.0050 - acc: 0.9994 - val_loss: 0.0659 - val_acc: 0.9830\n",
            "\n",
            "Epoch 00016: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.016.h5\n",
            "Epoch 17/20\n",
            "Learning Rate:  0.001\n",
            "54000/54000 [==============================] - 7s 123us/step - loss: 0.0045 - acc: 0.9995 - val_loss: 0.0658 - val_acc: 0.9825\n",
            "\n",
            "Epoch 00017: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.017.h5\n",
            "Epoch 18/20\n",
            "Learning Rate:  0.001\n",
            "54000/54000 [==============================] - 7s 123us/step - loss: 0.0044 - acc: 0.9995 - val_loss: 0.0658 - val_acc: 0.9823\n",
            "\n",
            "Epoch 00018: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.018.h5\n",
            "Epoch 19/20\n",
            "Learning Rate:  0.001\n",
            "54000/54000 [==============================] - 7s 123us/step - loss: 0.0044 - acc: 0.9995 - val_loss: 0.0658 - val_acc: 0.9823\n",
            "\n",
            "Epoch 00019: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.019.h5\n",
            "Epoch 20/20\n",
            "Learning Rate:  0.001\n",
            "54000/54000 [==============================] - 7s 123us/step - loss: 0.0044 - acc: 0.9995 - val_loss: 0.0659 - val_acc: 0.9822\n",
            "\n",
            "Epoch 00020: saving model to /usr/local/src/mnistsavedmodels/mnistmodel.020.h5\n",
            "10000/10000 [==============================] - 0s 45us/step\n",
            "Accuracy:  0.9809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F9ul2l3ffpvP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2264
        },
        "outputId": "c8b19fba-ee00-4d80-c554-37b66dcf4c6f"
      },
      "cell_type": "code",
      "source": [
        "# Convolutional Neural Networks\n",
        "# Import needed classes\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten\n",
        "from keras.models import Model,Input\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "\n",
        "# Load the mnist dataset\n",
        "(train_x, train_y) , (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "train_x = train_x.astype('float32') / 255\n",
        "test_x = test_x.astype('float32') / 255\n",
        "\n",
        "# Print the shapes of the data arrays\n",
        "print(\"Train Images: \",train_x.shape)\n",
        "print(\"Train Labels: \",train_y.shape)\n",
        "print(\"Test Images: \",test_x.shape)\n",
        "print(\"Test Labels: \",test_y.shape)\n",
        "\n",
        "# Reshape from (28,28) to (28,28,1)\n",
        "train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)\n",
        "test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)\n",
        "\n",
        "# Encode the labels to vectors\n",
        "train_y = keras.utils.to_categorical(train_y,10)\n",
        "test_y = keras.utils.to_categorical(test_y,10)\n",
        "\n",
        "# Define the model\n",
        "def MiniModel(input_shape):\n",
        "  images = Input(input_shape)\n",
        "  net = Conv2D(filters=64,kernel_size=[3,3],strides=[1,1],padding=\"same\",activation=\"relu\")(images)\n",
        "  net = Conv2D(filters=64,kernel_size=[3,3],strides=[1,1],padding=\"same\",activation=\"relu\")(net)\n",
        "  net = MaxPooling2D(pool_size=(2,2))(net)\n",
        "  net = Conv2D(filters=128,kernel_size=[3,3],strides=[1,1],padding=\"same\",activation=\"relu\")(net)\n",
        "  net = Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], padding=\"same\",activation=\"relu\")(net)\n",
        "  net = Flatten()(net)\n",
        "  net = Dense(units=10,activation=\"softmax\")(net)\n",
        "  model = Model(inputs=images,outputs=net)\n",
        "  return model\n",
        "\n",
        "input_shape = (28,28,1)\n",
        "model = MiniModel(input_shape)\n",
        "\n",
        "# Print a Summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Define the Learning rate schedule function\n",
        "def lr_schedule(epoch):\n",
        "  lr = 0.1\n",
        "  if epoch > 15:\n",
        "    lr = lr / 100\n",
        "  elif epoch > 10:\n",
        "    lr = lr / 10\n",
        "  elif epoch > 5:\n",
        "    lr = lr / 5\n",
        "  print(\"Learning Rate: \",lr)\n",
        "  return lr\n",
        "\n",
        "# Pass the scheduler function to the Learning Rate Scheduler class\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# Directory in which to create models\n",
        "save_direc = os.path.join(os.getcwd(), 'mnistsavedmodels')\n",
        "\n",
        "# Name of model files\n",
        "model_name = 'mnistmodel.{epoch:03d}.h5'\n",
        "\n",
        "# Create Directory if it doesn't exist\n",
        "if not os.path.isdir(save_direc):\n",
        "  os.makedirs(save_direc)\n",
        "\n",
        "# Join the directory with the model file\n",
        "modelpath = os.path.join(save_direc, model_name)\n",
        "checkpoint = ModelCheckpoint(filepath=modelpath,\n",
        " monitor='val_acc',\n",
        " verbose=1,\n",
        " save_best_only=True,\n",
        " period=1)\n",
        "\n",
        "# Specify the training components\n",
        "model.compile(optimizer=SGD(lr_schedule(0)),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(train_x,train_y,batch_size=32,epochs=20,shuffle=True,validation_split=0.1,verbose=1,callbacks=[checkpoint,lr_scheduler])\n",
        "\n",
        "# Evaluate the accuracy of the test dataset\n",
        "accuracy = model.evaluate(x=test_x,y=test_y,batch_size=32)\n",
        "print(\"Accuracy: \",accuracy[1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Images:  (60000, 28, 28)\n",
            "Train Labels:  (60000,)\n",
            "Test Images:  (10000, 28, 28)\n",
            "Test Labels:  (10000,)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 64)        640       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 14, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 14, 14, 128)       147584    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 10)                250890    \n",
            "=================================================================\n",
            "Total params: 509,898\n",
            "Trainable params: 509,898\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Learning Rate:  0.1\n",
            "Train on 54000 samples, validate on 6000 samples\n",
            "Epoch 1/20\n",
            "Learning Rate:  0.1\n",
            "54000/54000 [==============================] - 28s 523us/step - loss: 0.1653 - acc: 0.9495 - val_loss: 0.0530 - val_acc: 0.9845\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.98450, saving model to /usr/local/src/mnistsavedmodels/mnistmodel.001.h5\n",
            "Epoch 2/20\n",
            "Learning Rate:  0.1\n",
            "54000/54000 [==============================] - 26s 478us/step - loss: 0.0463 - acc: 0.9854 - val_loss: 0.2668 - val_acc: 0.9115\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.98450\n",
            "Epoch 3/20\n",
            "Learning Rate:  0.1\n",
            "54000/54000 [==============================] - 26s 478us/step - loss: 0.0317 - acc: 0.9904 - val_loss: 0.0388 - val_acc: 0.9895\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.98450 to 0.98950, saving model to /usr/local/src/mnistsavedmodels/mnistmodel.003.h5\n",
            "Epoch 4/20\n",
            "Learning Rate:  0.1\n",
            "54000/54000 [==============================] - 26s 478us/step - loss: 0.0232 - acc: 0.9925 - val_loss: 0.0471 - val_acc: 0.9868\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.98950\n",
            "Epoch 5/20\n",
            "Learning Rate:  0.1\n",
            "54000/54000 [==============================] - 26s 478us/step - loss: 0.0170 - acc: 0.9944 - val_loss: 0.0339 - val_acc: 0.9915\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.98950 to 0.99150, saving model to /usr/local/src/mnistsavedmodels/mnistmodel.005.h5\n",
            "Epoch 6/20\n",
            "Learning Rate:  0.1\n",
            "54000/54000 [==============================] - 26s 478us/step - loss: 0.0131 - acc: 0.9962 - val_loss: 0.0392 - val_acc: 0.9900\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.99150\n",
            "Epoch 7/20\n",
            "Learning Rate:  0.02\n",
            "54000/54000 [==============================] - 26s 477us/step - loss: 0.0047 - acc: 0.9990 - val_loss: 0.0335 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.99150 to 0.99200, saving model to /usr/local/src/mnistsavedmodels/mnistmodel.007.h5\n",
            "Epoch 8/20\n",
            "Learning Rate:  0.02\n",
            "54000/54000 [==============================] - 26s 477us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0346 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.99200 to 0.99217, saving model to /usr/local/src/mnistsavedmodels/mnistmodel.008.h5\n",
            "Epoch 9/20\n",
            "Learning Rate:  0.02\n",
            "54000/54000 [==============================] - 26s 476us/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.0360 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.99217\n",
            "Epoch 10/20\n",
            "Learning Rate:  0.02\n",
            "54000/54000 [==============================] - 26s 478us/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.0376 - val_acc: 0.9918\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.99217\n",
            "Epoch 11/20\n",
            "Learning Rate:  0.02\n",
            "54000/54000 [==============================] - 26s 476us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0394 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.99217 to 0.99233, saving model to /usr/local/src/mnistsavedmodels/mnistmodel.011.h5\n",
            "Epoch 12/20\n",
            "Learning Rate:  0.01\n",
            "54000/54000 [==============================] - 26s 475us/step - loss: 7.3709e-04 - acc: 0.9999 - val_loss: 0.0394 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.99233\n",
            "Epoch 13/20\n",
            "Learning Rate:  0.01\n",
            "54000/54000 [==============================] - 26s 478us/step - loss: 6.5866e-04 - acc: 0.9999 - val_loss: 0.0403 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.99233\n",
            "Epoch 14/20\n",
            "Learning Rate:  0.01\n",
            "54000/54000 [==============================] - 26s 478us/step - loss: 5.8027e-04 - acc: 0.9999 - val_loss: 0.0406 - val_acc: 0.9928\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.99233 to 0.99283, saving model to /usr/local/src/mnistsavedmodels/mnistmodel.014.h5\n",
            "Epoch 15/20\n",
            "Learning Rate:  0.01\n",
            "54000/54000 [==============================] - 26s 477us/step - loss: 5.0950e-04 - acc: 1.0000 - val_loss: 0.0425 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.99283\n",
            "Epoch 16/20\n",
            "Learning Rate:  0.01\n",
            "54000/54000 [==============================] - 26s 477us/step - loss: 4.6516e-04 - acc: 0.9999 - val_loss: 0.0423 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.99283\n",
            "Epoch 17/20\n",
            "Learning Rate:  0.001\n",
            "54000/54000 [==============================] - 26s 478us/step - loss: 3.9244e-04 - acc: 1.0000 - val_loss: 0.0423 - val_acc: 0.9920\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.99283\n",
            "Epoch 18/20\n",
            "Learning Rate:  0.001\n",
            "54000/54000 [==============================] - 26s 478us/step - loss: 3.8520e-04 - acc: 1.0000 - val_loss: 0.0423 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.99283\n",
            "Epoch 19/20\n",
            "Learning Rate:  0.001\n",
            "54000/54000 [==============================] - 26s 478us/step - loss: 3.8023e-04 - acc: 1.0000 - val_loss: 0.0423 - val_acc: 0.9922\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.99283\n",
            "Epoch 20/20\n",
            "Learning Rate:  0.001\n",
            "54000/54000 [==============================] - 26s 477us/step - loss: 3.7603e-04 - acc: 1.0000 - val_loss: 0.0423 - val_acc: 0.9923\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.99283\n",
            "10000/10000 [==============================] - 2s 160us/step\n",
            "Accuracy:  0.9912\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6q9NSJ6i0n9x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN with CIFAR10 Dataset"
      ]
    },
    {
      "metadata": {
        "id": "Xgse2too0a1a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2621
        },
        "outputId": "05cd86d4-c661-4f4f-911e-226e4cba5e31"
      },
      "cell_type": "code",
      "source": [
        "# CNN with CIFAR10 dataset\n",
        "# Import needed classes\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,AveragePooling2D,Dropout\n",
        "from keras.models import Model,Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "\n",
        "# Load the mnist dataset\n",
        "(train_x, train_y) , (test_x, test_y) = cifar10.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "train_x = train_x.astype('float32') / 255\n",
        "test_x = test_x.astype('float32') / 255\n",
        "\n",
        "# Print the shapes of the data arrays\n",
        "print(\"Train Images: \",train_x.shape)\n",
        "print(\"Train Labels: \",train_y.shape)\n",
        "print(\"Test Images: \",test_x.shape)\n",
        "print(\"Test Labels: \",test_y.shape)\n",
        "\n",
        "# Encode the labels to vectors\n",
        "train_y = keras.utils.to_categorical(train_y,10)\n",
        "test_y = keras.utils.to_categorical(test_y,10)\n",
        "\n",
        "# Define the model\n",
        "def MiniModel(input_shape):\n",
        "  images = Input(input_shape)\n",
        "  net = Conv2D(filters=64,kernel_size=[3,3],strides=[1,1],padding=\"same\",activation=\"relu\")(images)\n",
        "  net = Conv2D(filters=64,kernel_size=[3,3],strides=[1,1],padding=\"same\",activation=\"relu\")(net)\n",
        "  net = Conv2D(filters=64, kernel_size=[3, 3], strides=[1, 1], padding=\"same\",activation=\"relu\")(net)\n",
        "  net = MaxPooling2D(pool_size=(2,2))(net)\n",
        "  net = Conv2D(filters=128,kernel_size=[3,3],strides=[1,1],padding=\"same\",activation=\"relu\")(net)\n",
        "  net = Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], padding=\"same\",activation=\"relu\")(net)\n",
        "  net = Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], padding=\"same\",activation=\"relu\")(net)\n",
        "  net = MaxPooling2D(pool_size=(2, 2))(net)\n",
        "  net = Conv2D(filters=256, kernel_size=[3, 3], strides=[1, 1], padding=\"same\",activation=\"relu\")(net)\n",
        "  net = Conv2D(filters=256, kernel_size=[3, 3], strides=[1, 1], padding=\"same\",activation=\"relu\")(net)\n",
        "  net = Conv2D(filters=256, kernel_size=[3, 3], strides=[1, 1], padding=\"same\",activation=\"relu\")(net)\n",
        "  net = Dropout(0.25)(net)\n",
        "  net = AveragePooling2D(pool_size=(8,8))(net)\n",
        "  net = Flatten()(net)\n",
        "  net = Dense(units=10,activation=\"softmax\")(net)\n",
        "  model = Model(inputs=images,outputs=net)\n",
        "  return model\n",
        "\n",
        "input_shape = (32,32,3)\n",
        "model = MiniModel(input_shape)\n",
        "\n",
        "# Print a Summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Define the Learning rate schedule function\n",
        "def lr_schedule(epoch):\n",
        "  lr = 0.001\n",
        "  if epoch > 15:\n",
        "    lr = lr / 100\n",
        "  elif epoch > 10:\n",
        "    lr = lr / 10\n",
        "  elif epoch > 5:\n",
        "    lr = lr / 5\n",
        "  print(\"Learning Rate: \",lr)\n",
        "  return lr\n",
        "\n",
        "# Pass the scheduler function to the Learning Rate Scheduler class\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# Directory in which to create models\n",
        "save_direc = os.path.join(os.getcwd(), 'cifar10savedmodels')\n",
        "\n",
        "# Name of model files\n",
        "model_name = 'cifar10model.{epoch:03d}.h5'\n",
        "\n",
        "# Create Directory if it doesn't exist\n",
        "if not os.path.isdir(save_direc):\n",
        "  os.makedirs(save_direc)\n",
        "\n",
        "# Join the directory with the model file\n",
        "modelpath = os.path.join(save_direc, model_name)\n",
        "checkpoint = ModelCheckpoint(filepath=modelpath,\n",
        " monitor='val_acc',\n",
        " verbose=1,\n",
        " save_best_only=True,\n",
        " period=1)\n",
        "\n",
        "# Specify the training components\n",
        "model.compile(optimizer=Adam(lr_schedule(0)),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "#Fit the model\n",
        "model.fit(train_x,train_y,batch_size=128,epochs=20,shuffle=True,validation_split=0.1,verbose=1,callbacks=[checkpoint,lr_scheduler])\n",
        "\n",
        "# Evaluate the accuracy of the test dataset\n",
        "accuracy = model.evaluate(x=test_x,y=test_y,batch_size=128)\n",
        "print(\"Accuracy: \",accuracy[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 45s 0us/step\n",
            "Train Images:  (50000, 32, 32, 3)\n",
            "Train Labels:  (50000, 1)\n",
            "Test Images:  (10000, 32, 32, 3)\n",
            "Test Labels:  (10000, 1)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 1,922,570\n",
            "Trainable params: 1,922,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Learning Rate:  0.001\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/20\n",
            "Learning Rate:  0.001\n",
            "45000/45000 [==============================] - 36s 806us/step - loss: 2.1350 - acc: 0.1837 - val_loss: 1.7659 - val_acc: 0.3370\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.33700, saving model to /usr/local/src/cifar10savedmodels/cifar10model.001.h5\n",
            "Epoch 2/20\n",
            "Learning Rate:  0.001\n",
            "45000/45000 [==============================] - 34s 763us/step - loss: 1.5989 - acc: 0.4086 - val_loss: 1.4064 - val_acc: 0.4780\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.33700 to 0.47800, saving model to /usr/local/src/cifar10savedmodels/cifar10model.002.h5\n",
            "Epoch 3/20\n",
            "Learning Rate:  0.001\n",
            "45000/45000 [==============================] - 34s 765us/step - loss: 1.2857 - acc: 0.5336 - val_loss: 1.1630 - val_acc: 0.5804\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.47800 to 0.58040, saving model to /usr/local/src/cifar10savedmodels/cifar10model.003.h5\n",
            "Epoch 4/20\n",
            "Learning Rate:  0.001\n",
            "45000/45000 [==============================] - 34s 762us/step - loss: 1.0784 - acc: 0.6136 - val_loss: 1.0179 - val_acc: 0.6382\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.58040 to 0.63820, saving model to /usr/local/src/cifar10savedmodels/cifar10model.004.h5\n",
            "Epoch 5/20\n",
            "Learning Rate:  0.001\n",
            "45000/45000 [==============================] - 34s 766us/step - loss: 0.9119 - acc: 0.6750 - val_loss: 0.8404 - val_acc: 0.7006\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.63820 to 0.70060, saving model to /usr/local/src/cifar10savedmodels/cifar10model.005.h5\n",
            "Epoch 6/20\n",
            "Learning Rate:  0.001\n",
            "45000/45000 [==============================] - 34s 766us/step - loss: 0.7775 - acc: 0.7265 - val_loss: 0.7823 - val_acc: 0.7272\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.70060 to 0.72720, saving model to /usr/local/src/cifar10savedmodels/cifar10model.006.h5\n",
            "Epoch 7/20\n",
            "Learning Rate:  0.0002\n",
            "45000/45000 [==============================] - 35s 768us/step - loss: 0.5858 - acc: 0.7943 - val_loss: 0.6710 - val_acc: 0.7718\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.72720 to 0.77180, saving model to /usr/local/src/cifar10savedmodels/cifar10model.007.h5\n",
            "Epoch 8/20\n",
            "Learning Rate:  0.0002\n",
            "45000/45000 [==============================] - 34s 766us/step - loss: 0.5346 - acc: 0.8125 - val_loss: 0.6677 - val_acc: 0.7692\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.77180\n",
            "Epoch 9/20\n",
            "Learning Rate:  0.0002\n",
            "45000/45000 [==============================] - 34s 766us/step - loss: 0.4998 - acc: 0.8248 - val_loss: 0.6523 - val_acc: 0.7824\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.77180 to 0.78240, saving model to /usr/local/src/cifar10savedmodels/cifar10model.009.h5\n",
            "Epoch 10/20\n",
            "Learning Rate:  0.0002\n",
            "45000/45000 [==============================] - 34s 765us/step - loss: 0.4665 - acc: 0.8363 - val_loss: 0.6397 - val_acc: 0.7800\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.78240\n",
            "Epoch 11/20\n",
            "Learning Rate:  0.0002\n",
            "45000/45000 [==============================] - 34s 766us/step - loss: 0.4339 - acc: 0.8485 - val_loss: 0.6396 - val_acc: 0.7964\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.78240 to 0.79640, saving model to /usr/local/src/cifar10savedmodels/cifar10model.011.h5\n",
            "Epoch 12/20\n",
            "Learning Rate:  0.0001\n",
            "45000/45000 [==============================] - 34s 766us/step - loss: 0.3833 - acc: 0.8684 - val_loss: 0.6383 - val_acc: 0.7994\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.79640 to 0.79940, saving model to /usr/local/src/cifar10savedmodels/cifar10model.012.h5\n",
            "Epoch 13/20\n",
            "Learning Rate:  0.0001\n",
            "45000/45000 [==============================] - 35s 767us/step - loss: 0.3620 - acc: 0.8760 - val_loss: 0.6578 - val_acc: 0.7888\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.79940\n",
            "Epoch 14/20\n",
            "Learning Rate:  0.0001\n",
            "45000/45000 [==============================] - 34s 766us/step - loss: 0.3451 - acc: 0.8810 - val_loss: 0.6441 - val_acc: 0.7928\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.79940\n",
            "Epoch 15/20\n",
            "Learning Rate:  0.0001\n",
            "45000/45000 [==============================] - 34s 766us/step - loss: 0.3265 - acc: 0.8878 - val_loss: 0.6537 - val_acc: 0.7972\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.79940\n",
            "Epoch 16/20\n",
            "Learning Rate:  0.0001\n",
            "45000/45000 [==============================] - 34s 766us/step - loss: 0.3097 - acc: 0.8935 - val_loss: 0.6636 - val_acc: 0.7966\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.79940\n",
            "Epoch 17/20\n",
            "Learning Rate:  1e-05\n",
            "45000/45000 [==============================] - 34s 764us/step - loss: 0.2732 - acc: 0.9100 - val_loss: 0.6652 - val_acc: 0.7992\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.79940\n",
            "Epoch 18/20\n",
            "Learning Rate:  1e-05\n",
            "45000/45000 [==============================] - 34s 765us/step - loss: 0.2680 - acc: 0.9108 - val_loss: 0.6693 - val_acc: 0.8000\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.79940 to 0.80000, saving model to /usr/local/src/cifar10savedmodels/cifar10model.018.h5\n",
            "Epoch 19/20\n",
            "Learning Rate:  1e-05\n",
            "45000/45000 [==============================] - 34s 763us/step - loss: 0.2651 - acc: 0.9124 - val_loss: 0.6748 - val_acc: 0.7972\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.80000\n",
            "Epoch 20/20\n",
            "Learning Rate:  1e-05\n",
            "45000/45000 [==============================] - 34s 766us/step - loss: 0.2631 - acc: 0.9137 - val_loss: 0.6782 - val_acc: 0.7978\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.80000\n",
            "10000/10000 [==============================] - 2s 246us/step\n",
            "Accuracy:  0.7877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CXlh_7zH4UXJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#import needed classes\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,AveragePooling2D,Dropout,BatchNormalization,Activation\n",
        "from keras.models import Model,Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "\n",
        "#load the mnist dataset\n",
        "(train_x, train_y) , (test_x, test_y) = cifar10.load_data()\n",
        "\n",
        "#normalize the data\n",
        "train_x = train_x.astype('float32') / 255\n",
        "test_x = test_x.astype('float32') / 255\n",
        "\n",
        "#print the shapes of the data arrays\n",
        "print(\"Train Images: \",train_x.shape)\n",
        "print(\"Train Labels: \",train_y.shape)\n",
        "print(\"Test Images: \",test_x.shape)\n",
        "print(\"Test Labels: \",test_y.shape)\n",
        "\n",
        "#Encode the labels to vectors\n",
        "train_y = keras.utils.to_categorical(train_y,10)\n",
        "test_y = keras.utils.to_categorical(test_y,10)\n",
        "\n",
        "#Define the model\n",
        "def MiniModel(input_shape):\n",
        "  images = Input(input_shape)\n",
        "  net = BatchNormalization()(images)\n",
        "  net = Activation(\"relu\")(net)\n",
        "  net = Conv2D(filters=64,kernel_size=[3,3],strides=[1,1],padding=\"same\")(net)\n",
        "  net = BatchNormalization()(net)\n",
        "  net = Activation(\"relu\")(net)\n",
        "  net = Conv2D(filters=64,kernel_size=[3,3],strides=[1,1],padding=\"same\")(net)\n",
        "  net = BatchNormalization()(net)\n",
        "  net = Activation(\"relu\")(net)\n",
        "  net = Conv2D(filters=64, kernel_size=[3, 3], strides=[1, 1], padding=\"same\",activation=\"relu\")(net)\n",
        "  net = MaxPooling2D(pool_size=(2,2))(net)\n",
        "  net = BatchNormalization()(net)\n",
        "  net = Activation(\"relu\")(net)\n",
        "  net = Conv2D(filters=128,kernel_size=[3,3],strides=[1,1],padding=\"same\")(net)\n",
        "  net = BatchNormalization()(net)\n",
        "  net = Activation(\"relu\")(net)\n",
        "  net = Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], padding=\"same\",activation=\"relu\")(net)\n",
        "  net = BatchNormalization()(net)\n",
        "  net = Activation(\"relu\")(net)\n",
        "  net = Conv2D(filters=128, kernel_size=[3, 3], strides=[1, 1], padding=\"same\",activation=\"relu\")(net)\n",
        "  net = MaxPooling2D(pool_size=(2, 2))(net)\n",
        "  net = BatchNormalization()(net)\n",
        "  net = Activation(\"relu\")(net)\n",
        "  net = Conv2D(filters=256, kernel_size=[3, 3], strides=[1, 1], padding=\"same\",activation=\"relu\")(net)\n",
        "  net = BatchNormalization()(net)\n",
        "  net = Activation(\"relu\")(net)\n",
        "  net = Conv2D(filters=256, kernel_size=[3, 3], strides=[1, 1], padding=\"same\",activation=\"relu\")(net)\n",
        "  net = BatchNormalization()(net)\n",
        "  net = Activation(\"relu\")(net)\n",
        "  net = Conv2D(filters=256, kernel_size=[3, 3], strides=[1, 1], padding=\"same\",activation=\"relu\")(net)\n",
        "  net = Dropout(0.25)(net)\n",
        "  net = AveragePooling2D(pool_size=(8,8))(net)\n",
        "  net = Flatten()(net)\n",
        "  net = Dense(units=10,activation=\"softmax\")(net)\n",
        "  model = Model(inputs=images,outputs=net)\n",
        "  return model\n",
        "\n",
        "input_shape = (32,32,3)\n",
        "model = MiniModel(input_shape)\n",
        "\n",
        "#Print a Summary of the model\n",
        "model.summary()\n",
        "\n",
        "#Define the Learning rate schedule function\n",
        "def lr_schedule(epoch):\n",
        "  lr = 0.001\n",
        "  if epoch > 15:\n",
        "    lr = lr / 100\n",
        "  elif epoch > 10:\n",
        "    lr = lr / 10\n",
        "  elif epoch > 5:\n",
        "    lr = lr / 5\n",
        "  print(\"Learning Rate: \",lr)\n",
        "  return lr\n",
        "\n",
        "#Pass the scheduler function to the Learning Rate Scheduler class\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "#Directory in which to create models\n",
        "save_direc = os.path.join(os.getcwd(), 'cifar10savedmodels')\n",
        "\n",
        "#Name of model files\n",
        "model_name = 'cifar10model.{epoch:03d}.h5'\n",
        "\n",
        "#Create Directory if it doesn't exist\n",
        "if not os.path.isdir(save_direc):\n",
        "  os.makedirs(save_direc)\n",
        "#Join the directory with the model file\n",
        "modelpath = os.path.join(save_direc, model_name)\n",
        "checkpoint = ModelCheckpoint(filepath=modelpath,\n",
        " monitor='val_acc',\n",
        " verbose=1,\n",
        " save_best_only=True,\n",
        " period=1)\n",
        "\n",
        "#Specify the training components\n",
        "model.compile(optimizer=Adam(lr_schedule(0)),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "#Fit the model\n",
        "model.fit(train_x,train_y,batch_size=128,epochs=20,shuffle=True,validation_split=0.1,verbose=1,callbacks=[checkpoint,lr_scheduler])\n",
        "\n",
        "#Evaluate the accuracy of the test dataset\n",
        "accuracy = model.evaluate(x=test_x,y=test_y,batch_size=128)\n",
        "print(\"Accuracy: \",accuracy[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RiZOMmdm5Vu6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3148
        },
        "outputId": "d06a1de5-179f-48dd-9960-b55e83c9ca16"
      },
      "cell_type": "code",
      "source": [
        "#import needed classes\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,AveragePooling2D,Dropout,BatchNormalization,Activation\n",
        "from keras.models import Model,Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "\n",
        "#load the mnist dataset\n",
        "(train_x, train_y) , (test_x, test_y) = cifar10.load_data()\n",
        "\n",
        "#normalize the data\n",
        "train_x = train_x.astype('float32') / 255\n",
        "test_x = test_x.astype('float32') / 255\n",
        "\n",
        "#print the shapes of the data arrays\n",
        "print(\"Train Images: \",train_x.shape)\n",
        "print(\"Train Labels: \",train_y.shape)\n",
        "print(\"Test Images: \",test_x.shape)\n",
        "print(\"Test Labels: \",test_y.shape)\n",
        "\n",
        "#Encode the labels to vectors\n",
        "train_y = keras.utils.to_categorical(train_y,10)\n",
        "test_y = keras.utils.to_categorical(test_y,10)\n",
        "\n",
        "#define a common unit\n",
        "def Unit(x,filters):\n",
        "  out = BatchNormalization()(x)\n",
        "  out = Activation(\"relu\")(out)\n",
        "  out = Conv2D(filters=filters, kernel_size=[3, 3], strides=[1, 1],padding=\"same\")(out)\n",
        "  return out\n",
        "\n",
        "def MiniModel(input_shape):\n",
        "  images = Input(input_shape)\n",
        "  net = Unit(images,64)\n",
        "  net = Unit(net,64)\n",
        "  net = Unit(net,64)\n",
        "  net = MaxPooling2D(pool_size=(2,2))(net)\n",
        "  net = Unit(net,128)\n",
        "  net = Unit(net,128)\n",
        "  net = Unit(net,128)\n",
        "  net = MaxPooling2D(pool_size=(2, 2))(net)\n",
        "  net = Unit(net,256)\n",
        "  net = Unit(net,256)\n",
        "  net = Unit(net,256)\n",
        "  net = Dropout(0.25)(net)\n",
        "  net = AveragePooling2D(pool_size=(8,8))(net)\n",
        "  net = Flatten()(net)\n",
        "  net = Dense(units=10,activation=\"softmax\")(net)\n",
        "  model = Model(inputs=images,outputs=net)\n",
        "  return model\n",
        "\n",
        "input_shape = (32,32,3)\n",
        "model = MiniModel(input_shape)\n",
        "\n",
        "#Print a Summary of the model\n",
        "model.summary()\n",
        "\n",
        "#Define the Learning rate schedule function\n",
        "def lr_schedule(epoch):\n",
        "  lr = 0.001\n",
        "  if epoch > 15:\n",
        "    lr = lr / 100\n",
        "  elif epoch > 10:\n",
        "    lr = lr / 10\n",
        "  elif epoch > 5:\n",
        "    lr = lr / 5\n",
        "  print(\"Learning Rate: \",lr)\n",
        "  return lr\n",
        "\n",
        "#Pass the scheduler function to the Learning Rate Scheduler class\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "#Directory in which to create models\n",
        "save_direc = os.path.join(os.getcwd(), 'cifar10savedmodels')\n",
        "\n",
        "#Name of model files\n",
        "model_name = 'cifar10model.{epoch:03d}.h5'\n",
        "\n",
        "#Create Directory if it doesn't exist\n",
        "if not os.path.isdir(save_direc):\n",
        "  os.makedirs(save_direc)\n",
        "#Join the directory with the model file\n",
        "modelpath = os.path.join(save_direc, model_name)\n",
        "checkpoint = ModelCheckpoint(filepath=modelpath,\n",
        " monitor='val_acc',\n",
        " verbose=1,\n",
        " save_best_only=True,\n",
        " period=1)\n",
        "\n",
        "#Specify the training components\n",
        "model.compile(optimizer=Adam(lr_schedule(0)),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "\n",
        "#Fit the model\n",
        "model.fit(train_x,train_y,batch_size=128,epochs=20,shuffle=True,validation_split=0.1,verbose=1,callbacks=[checkpoint,lr_scheduler])\n",
        "\n",
        "#Evaluate the accuracy of the test dataset\n",
        "accuracy = model.evaluate(x=test_x,y=test_y,batch_size=128)\n",
        "print(\"Accuracy: \",accuracy[1])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Images:  (50000, 32, 32, 3)\n",
            "Train Labels:  (50000, 1)\n",
            "Test Images:  (10000, 32, 32, 3)\n",
            "Test Labels:  (10000, 1)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 32, 32, 3)         12        \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_3 (Average (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 1,926,934\n",
            "Trainable params: 1,924,752\n",
            "Non-trainable params: 2,182\n",
            "_________________________________________________________________\n",
            "Learning Rate:  0.001\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/20\n",
            "Learning Rate:  0.001\n",
            "45000/45000 [==============================] - 47s 1ms/step - loss: 2.1883 - acc: 0.2718 - val_loss: 2.6881 - val_acc: 0.1896\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.18960, saving model to /usr/local/src/cifar10savedmodels/cifar10model.001.h5\n",
            "Epoch 2/20\n",
            "Learning Rate:  0.001\n",
            "45000/45000 [==============================] - 45s 992us/step - loss: 1.5341 - acc: 0.4303 - val_loss: 1.5014 - val_acc: 0.4380\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.18960 to 0.43800, saving model to /usr/local/src/cifar10savedmodels/cifar10model.002.h5\n",
            "Epoch 3/20\n",
            "Learning Rate:  0.001\n",
            "45000/45000 [==============================] - 45s 990us/step - loss: 1.2457 - acc: 0.5502 - val_loss: 1.3197 - val_acc: 0.5202\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.43800 to 0.52020, saving model to /usr/local/src/cifar10savedmodels/cifar10model.003.h5\n",
            "Epoch 4/20\n",
            "Learning Rate:  0.001\n",
            "45000/45000 [==============================] - 45s 991us/step - loss: 1.0595 - acc: 0.6201 - val_loss: 1.0988 - val_acc: 0.5968\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.52020 to 0.59680, saving model to /usr/local/src/cifar10savedmodels/cifar10model.004.h5\n",
            "Epoch 5/20\n",
            "Learning Rate:  0.001\n",
            "45000/45000 [==============================] - 45s 993us/step - loss: 0.9122 - acc: 0.6798 - val_loss: 0.9226 - val_acc: 0.6726\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.59680 to 0.67260, saving model to /usr/local/src/cifar10savedmodels/cifar10model.005.h5\n",
            "Epoch 6/20\n",
            "Learning Rate:  0.001\n",
            "45000/45000 [==============================] - 45s 994us/step - loss: 0.7591 - acc: 0.7314 - val_loss: 1.0553 - val_acc: 0.6486\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.67260\n",
            "Epoch 7/20\n",
            "Learning Rate:  0.0002\n",
            "45000/45000 [==============================] - 45s 994us/step - loss: 0.5405 - acc: 0.8114 - val_loss: 0.6420 - val_acc: 0.7800\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.67260 to 0.78000, saving model to /usr/local/src/cifar10savedmodels/cifar10model.007.h5\n",
            "Epoch 8/20\n",
            "Learning Rate:  0.0002\n",
            "45000/45000 [==============================] - 45s 993us/step - loss: 0.4810 - acc: 0.8323 - val_loss: 0.5948 - val_acc: 0.7964\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.78000 to 0.79640, saving model to /usr/local/src/cifar10savedmodels/cifar10model.008.h5\n",
            "Epoch 9/20\n",
            "Learning Rate:  0.0002\n",
            "45000/45000 [==============================] - 45s 994us/step - loss: 0.4340 - acc: 0.8486 - val_loss: 0.6413 - val_acc: 0.7886\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.79640\n",
            "Epoch 10/20\n",
            "Learning Rate:  0.0002\n",
            "45000/45000 [==============================] - 45s 992us/step - loss: 0.3914 - acc: 0.8621 - val_loss: 0.6077 - val_acc: 0.7992\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.79640 to 0.79920, saving model to /usr/local/src/cifar10savedmodels/cifar10model.010.h5\n",
            "Epoch 11/20\n",
            "Learning Rate:  0.0002\n",
            "45000/45000 [==============================] - 45s 993us/step - loss: 0.3495 - acc: 0.8769 - val_loss: 0.7886 - val_acc: 0.7606\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.79920\n",
            "Epoch 12/20\n",
            "Learning Rate:  0.0001\n",
            "45000/45000 [==============================] - 45s 993us/step - loss: 0.2760 - acc: 0.9038 - val_loss: 0.6497 - val_acc: 0.8078\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.79920 to 0.80780, saving model to /usr/local/src/cifar10savedmodels/cifar10model.012.h5\n",
            "Epoch 13/20\n",
            "Learning Rate:  0.0001\n",
            "45000/45000 [==============================] - 45s 993us/step - loss: 0.2450 - acc: 0.9150 - val_loss: 0.6496 - val_acc: 0.8134\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.80780 to 0.81340, saving model to /usr/local/src/cifar10savedmodels/cifar10model.013.h5\n",
            "Epoch 14/20\n",
            "Learning Rate:  0.0001\n",
            "45000/45000 [==============================] - 45s 994us/step - loss: 0.2195 - acc: 0.9226 - val_loss: 0.6683 - val_acc: 0.8186\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.81340 to 0.81860, saving model to /usr/local/src/cifar10savedmodels/cifar10model.014.h5\n",
            "Epoch 15/20\n",
            "Learning Rate:  0.0001\n",
            "45000/45000 [==============================] - 45s 991us/step - loss: 0.1938 - acc: 0.9321 - val_loss: 0.7539 - val_acc: 0.8128\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.81860\n",
            "Epoch 16/20\n",
            "Learning Rate:  0.0001\n",
            "45000/45000 [==============================] - 44s 987us/step - loss: 0.1711 - acc: 0.9392 - val_loss: 0.7423 - val_acc: 0.8158\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.81860\n",
            "Epoch 17/20\n",
            "Learning Rate:  1e-05\n",
            "45000/45000 [==============================] - 44s 986us/step - loss: 0.1178 - acc: 0.9619 - val_loss: 0.7243 - val_acc: 0.8242\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.81860 to 0.82420, saving model to /usr/local/src/cifar10savedmodels/cifar10model.017.h5\n",
            "Epoch 18/20\n",
            "Learning Rate:  1e-05\n",
            "45000/45000 [==============================] - 45s 990us/step - loss: 0.1099 - acc: 0.9641 - val_loss: 0.7458 - val_acc: 0.8264\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.82420 to 0.82640, saving model to /usr/local/src/cifar10savedmodels/cifar10model.018.h5\n",
            "Epoch 19/20\n",
            "Learning Rate:  1e-05\n",
            "45000/45000 [==============================] - 45s 990us/step - loss: 0.1034 - acc: 0.9668 - val_loss: 0.7719 - val_acc: 0.8244\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.82640\n",
            "Epoch 20/20\n",
            "Learning Rate:  1e-05\n",
            "45000/45000 [==============================] - 45s 990us/step - loss: 0.1006 - acc: 0.9671 - val_loss: 0.7827 - val_acc: 0.8258\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.82640\n",
            "10000/10000 [==============================] - 3s 282us/step\n",
            "Accuracy:  0.8146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Up1uSUdv55yw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation Example"
      ]
    },
    {
      "metadata": {
        "id": "zFNBZg0y56BR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3114
        },
        "outputId": "bc2ff61e-8610-429c-fb08-47c0b15560a9"
      },
      "cell_type": "code",
      "source": [
        "#import needed classes\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,AveragePooling2D,Dropout,BatchNormalization,Activation\n",
        "from keras.models import Model,Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from math import ceil\n",
        "import os\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#load the mnist dataset\n",
        "(train_x, train_y) , (test_x, test_y) = cifar10.load_data()\n",
        "\n",
        "#normalize the data\n",
        "train_x = train_x.astype('float32') / 255\n",
        "test_x = test_x.astype('float32') / 255\n",
        "\n",
        "#Subtract the mean image from both train and test set\n",
        "train_x = train_x - train_x.mean()\n",
        "test_x = test_x - test_x.mean()\n",
        "\n",
        "#Divide by the standard deviation\n",
        "train_x = train_x / train_x.std(axis=0)\n",
        "test_x = test_x / test_x.std(axis=0)\n",
        "\n",
        "#print the shapes of the data arrays\n",
        "print(\"Train Images: \",train_x.shape)\n",
        "print(\"Train Labels: \",train_y.shape)\n",
        "print(\"Test Images: \",test_x.shape)\n",
        "print(\"Test Labels: \",test_y.shape)\n",
        "\n",
        "#Encode the labels to vectors\n",
        "train_y = keras.utils.to_categorical(train_y,10)\n",
        "test_y = keras.utils.to_categorical(test_y,10)\n",
        "\n",
        "#define a common unit\n",
        "def Unit(x,filters):\n",
        "  out = BatchNormalization()(x)\n",
        "  out = Activation(\"relu\")(out)\n",
        "  out = Conv2D(filters=filters, kernel_size=[3, 3], strides=[1, 1],padding=\"same\")(out)\n",
        "  return out\n",
        "\n",
        "#Define the model\n",
        "def MiniModel(input_shape):\n",
        "  images = Input(input_shape)\n",
        "  net = Unit(images,64)\n",
        "  net = Unit(net,64)\n",
        "  net = Unit(net,64)\n",
        "  net = MaxPooling2D(pool_size=(2,2))(net)\n",
        "  net = Unit(net,128)\n",
        "  net = Unit(net,128)\n",
        "  net = Unit(net,128)\n",
        "  net = MaxPooling2D(pool_size=(2, 2))(net)\n",
        "  net = Unit(net,256)\n",
        "  net = Unit(net,256)\n",
        "  net = Unit(net,256)\n",
        "  net = Dropout(0.25)(net)\n",
        "  net = AveragePooling2D(pool_size=(8,8))(net)\n",
        "  net = Flatten()(net)\n",
        "  net = Dense(units=10,activation=\"softmax\")(net)\n",
        "  model = Model(inputs=images,outputs=net)\n",
        "  return model\n",
        "\n",
        "input_shape = (32,32,3)\n",
        "model = MiniModel(input_shape)\n",
        "\n",
        "#Print a Summary of the model\n",
        "model.summary()\n",
        "\n",
        "#Define the Learning rate schedule function\n",
        "def lr_schedule(epoch):\n",
        "  lr = 0.001\n",
        "  if epoch > 15:\n",
        "    lr = lr / 100\n",
        "  elif epoch > 10:\n",
        "    lr = lr / 10\n",
        "  elif epoch > 5:\n",
        "    lr = lr / 5\n",
        "  print(\"Learning Rate: \",lr)\n",
        "  return lr\n",
        "\n",
        "#Pass the scheduler function to the Learning Rate Scheduler class\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "#Directory in which to create models\n",
        "save_direc = os.path.join(os.getcwd(), 'cifar10savedmodels')\n",
        "\n",
        "#Name of model files\n",
        "model_name = 'cifar10model.{epoch:03d}.h5'\n",
        "\n",
        "#Create Directory if it doesn't exist\n",
        "if not os.path.isdir(save_direc):\n",
        "  os.makedirs(save_direc)\n",
        "\n",
        "#Join the directory with the model file\n",
        "modelpath = os.path.join(save_direc, model_name)\n",
        "checkpoint = ModelCheckpoint(filepath=modelpath,\n",
        " monitor='val_acc',\n",
        " verbose=1,\n",
        " save_best_only=True,\n",
        " period=1)\n",
        "\n",
        "#Specify the training components\n",
        "model.compile(optimizer=Adam(lr_schedule(0)),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
        "datagen = ImageDataGenerator(rotation_range=10,\n",
        " width_shift_range=5. / 32,\n",
        " height_shift_range=5. / 32,\n",
        " horizontal_flip=True)\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(train_x)\n",
        "epochs = 20\n",
        "steps_per_epoch = ceil(50000/128)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(train_x, train_y, batch_size=128),\n",
        " validation_data=[test_x,test_y],\n",
        "epochs=epochs,steps_per_epoch=steps_per_epoch, verbose=1,\n",
        "workers=4,\n",
        " callbacks=[ checkpoint,lr_scheduler])\n",
        "\n",
        "#Evaluate the accuracy of the test dataset\n",
        "accuracy = model.evaluate(x=test_x,y=test_y,batch_size=128)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Images:  (50000, 32, 32, 3)\n",
            "Train Labels:  (50000, 1)\n",
            "Test Images:  (10000, 32, 32, 3)\n",
            "Test Labels:  (10000, 1)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 32, 32, 3)         12        \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 32, 32, 64)        1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 32, 32, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 32, 32, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 16, 16, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 16, 16, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 8, 8, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 8, 8, 256)         590080    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_4 (Average (None, 1, 1, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 1,926,934\n",
            "Trainable params: 1,924,752\n",
            "Non-trainable params: 2,182\n",
            "_________________________________________________________________\n",
            "Learning Rate:  0.001\n",
            "Epoch 1/20\n",
            "Learning Rate:  0.001\n",
            "391/391 [==============================] - 57s 147ms/step - loss: 1.9254 - acc: 0.3162 - val_loss: 1.9652 - val_acc: 0.3038\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.30380, saving model to /usr/local/src/cifar10savedmodels/cifar10model.001.h5\n",
            "Epoch 2/20\n",
            "Learning Rate:  0.001\n",
            "391/391 [==============================] - 53s 135ms/step - loss: 1.4167 - acc: 0.4826 - val_loss: 1.6313 - val_acc: 0.4822\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.30380 to 0.48220, saving model to /usr/local/src/cifar10savedmodels/cifar10model.002.h5\n",
            "Epoch 3/20\n",
            "Learning Rate:  0.001\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 1.1514 - acc: 0.5871 - val_loss: 1.4735 - val_acc: 0.5294\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.48220 to 0.52940, saving model to /usr/local/src/cifar10savedmodels/cifar10model.003.h5\n",
            "Epoch 4/20\n",
            "Learning Rate:  0.001\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 0.9463 - acc: 0.6675 - val_loss: 0.8978 - val_acc: 0.6904\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.52940 to 0.69040, saving model to /usr/local/src/cifar10savedmodels/cifar10model.004.h5\n",
            "Epoch 5/20\n",
            "Learning Rate:  0.001\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 0.7809 - acc: 0.7279 - val_loss: 0.8648 - val_acc: 0.7187\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.69040 to 0.71870, saving model to /usr/local/src/cifar10savedmodels/cifar10model.005.h5\n",
            "Epoch 6/20\n",
            "Learning Rate:  0.001\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 0.6712 - acc: 0.7691 - val_loss: 0.8459 - val_acc: 0.7324\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.71870 to 0.73240, saving model to /usr/local/src/cifar10savedmodels/cifar10model.006.h5\n",
            "Epoch 7/20\n",
            "Learning Rate:  0.0002\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 0.5026 - acc: 0.8256 - val_loss: 0.5820 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.73240 to 0.80740, saving model to /usr/local/src/cifar10savedmodels/cifar10model.007.h5\n",
            "Epoch 8/20\n",
            "Learning Rate:  0.0002\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 0.4669 - acc: 0.8386 - val_loss: 0.6329 - val_acc: 0.7941\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.80740\n",
            "Epoch 9/20\n",
            "Learning Rate:  0.0002\n",
            "391/391 [==============================] - 52s 133ms/step - loss: 0.4381 - acc: 0.8489 - val_loss: 0.5374 - val_acc: 0.8199\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.80740 to 0.81990, saving model to /usr/local/src/cifar10savedmodels/cifar10model.009.h5\n",
            "Epoch 10/20\n",
            "Learning Rate:  0.0002\n",
            "391/391 [==============================] - 53s 134ms/step - loss: 0.4235 - acc: 0.8533 - val_loss: 0.5810 - val_acc: 0.8161\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.81990\n",
            "Epoch 11/20\n",
            "Learning Rate:  0.0002\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 0.4045 - acc: 0.8605 - val_loss: 0.5540 - val_acc: 0.8254\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.81990 to 0.82540, saving model to /usr/local/src/cifar10savedmodels/cifar10model.011.h5\n",
            "Epoch 12/20\n",
            "Learning Rate:  0.0001\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 0.3698 - acc: 0.8722 - val_loss: 0.4857 - val_acc: 0.8426\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.82540 to 0.84260, saving model to /usr/local/src/cifar10savedmodels/cifar10model.012.h5\n",
            "Epoch 13/20\n",
            "Learning Rate:  0.0001\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 0.3610 - acc: 0.8750 - val_loss: 0.5202 - val_acc: 0.8371\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.84260\n",
            "Epoch 14/20\n",
            "Learning Rate:  0.0001\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 0.3443 - acc: 0.8791 - val_loss: 0.5623 - val_acc: 0.8295\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.84260\n",
            "Epoch 15/20\n",
            "Learning Rate:  0.0001\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 0.3379 - acc: 0.8825 - val_loss: 0.4604 - val_acc: 0.8506\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.84260 to 0.85060, saving model to /usr/local/src/cifar10savedmodels/cifar10model.015.h5\n",
            "Epoch 16/20\n",
            "Learning Rate:  0.0001\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 0.3284 - acc: 0.8863 - val_loss: 0.5260 - val_acc: 0.8378\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.85060\n",
            "Epoch 17/20\n",
            "Learning Rate:  1e-05\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 0.3079 - acc: 0.8920 - val_loss: 0.4616 - val_acc: 0.8540\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.85060 to 0.85400, saving model to /usr/local/src/cifar10savedmodels/cifar10model.017.h5\n",
            "Epoch 18/20\n",
            "Learning Rate:  1e-05\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 0.2990 - acc: 0.8953 - val_loss: 0.4591 - val_acc: 0.8562\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.85400 to 0.85620, saving model to /usr/local/src/cifar10savedmodels/cifar10model.018.h5\n",
            "Epoch 19/20\n",
            "Learning Rate:  1e-05\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 0.2999 - acc: 0.8958 - val_loss: 0.4590 - val_acc: 0.8559\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.85620\n",
            "Epoch 20/20\n",
            "Learning Rate:  1e-05\n",
            "391/391 [==============================] - 52s 134ms/step - loss: 0.2965 - acc: 0.8970 - val_loss: 0.4653 - val_acc: 0.8550\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.85620\n",
            "10000/10000 [==============================] - 3s 283us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LKaI-yOZ_oPI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Basic ResNet Module Example"
      ]
    },
    {
      "metadata": {
        "id": "cKwUXsk3_oXK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ResnetModule(x,filters):\n",
        "  res = x\n",
        "  out = BatchNormalization()(x)\n",
        "  out = Activation(\"relu\")(out)\n",
        "  out = Conv2D(filters=filters, kernel_size=[3, 3], strides=[1, 1],padding=\"same\")(out)\n",
        "  out = BatchNormalization()(out)\n",
        "  out = Activation(\"relu\")(out)\n",
        "  out = Conv2D(filters=filters, kernel_size=[3, 3], strides=[1, 1],padding=\"same\")(out)\n",
        "  out = keras.layers.add([res,out])\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8gSYKv8JAWLH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ResNet Module with Pooling"
      ]
    },
    {
      "metadata": {
        "id": "4b5beA95AWUD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ResnetModule(x,filters,pool=False):\n",
        "  res = x\n",
        "  if pool:\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  res = Conv2D(filters=filters,kernel_size=[1,1],strides=(2,2),padding=\"same\")(res)\n",
        "  out = BatchNormalization()(x)\n",
        "  out = Activation(\"relu\")(out)\n",
        "  out = Conv2D(filters=filters, kernel_size=[3, 3], strides=[1, 1], padding=\"same\")(out)\n",
        "  out = BatchNormalization()(out)\n",
        "  out = Activation(\"relu\")(out)\n",
        "  out = Conv2D(filters=filters, kernel_size=[3, 3], strides=[1, 1], padding=\"same\")(out)\n",
        "  out = keras.layers.add([res,out])\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7_NTXfzTAj_4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Residual Module using ResNet w/ Pooling\n"
      ]
    },
    {
      "metadata": {
        "id": "Va6xcN9mAkI2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def MiniModel(input_shape):\n",
        "  images = Input(input_shape)\n",
        "  net = Conv2D(filters=32, kernel_size=[3, 3], strides=[1, 1],padding=\"same\")(images)\n",
        "  net = ResnetModule(net,32)\n",
        "  net = ResnetModule(net,32)\n",
        "  net = ResnetModule(net,32)\n",
        "  net = ResnetModule(net,64,pool=True)\n",
        "  net = ResnetModule(net,64)\n",
        "  net = ResnetModule(net,64)\n",
        "  net = ResnetModule(net,128,pool=True)\n",
        "  net = ResnetModule(net,128)\n",
        "  net = ResnetModule(net,128)\n",
        "  net = ResnetModule(net, 256,pool=True)\n",
        "  net = ResnetModule(net, 256)\n",
        "  net = ResnetModule(net, 256)\n",
        "  net = BatchNormalization()(net)\n",
        "  net = Activation(\"relu\")(net)\n",
        "  net = Dropout(0.25)(net)\n",
        "  net = AveragePooling2D(pool_size=(4,4))(net)\n",
        "  net = Flatten()(net)\n",
        "  net = Dense(units=10,activation=\"softmax\")(net)\n",
        "  model = Model(inputs=images,outputs=net)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5fwgAtvhB536",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Squeeze and Excitation (SE) Networks"
      ]
    },
    {
      "metadata": {
        "id": "KfIN4FEHB58s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def SEModule(x,filters,compress_ratio=16):\n",
        "  compression = int(filters/compress_ratio)\n",
        "  se = GlobalAveragePooling2D()(x)\n",
        "  se = Dense(compression)(se)\n",
        "  se = Activation(\"relu\")(se)\n",
        "  se = Dense(channels)(se)\n",
        "  se = Activation(\"sigmoid\")(se)\n",
        "  out = multiply([x, se])\n",
        "\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PKXBOxP9jR8E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Depth Wise Separable Convolutions\n",
        "**Mobile Netowrk Example**"
      ]
    },
    {
      "metadata": {
        "id": "yYdcyI7pjRSK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create the Main Module\n",
        "def Unit(x,filters,pool=False):\n",
        "  res = x\n",
        "  if pool:\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  res = Conv2D(filters=filters,kernel_size=[1,1],strides=(2,2),padding=\"same\")(res)\n",
        "  out = BatchNormalization()(x)\n",
        "  out = Activation(\"relu\")(out)\n",
        "  out = DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], padding=\"same\")(out)\n",
        "  out = BatchNormalization()(out)\n",
        "  out = Activation(\"relu\")(out)\n",
        "  out = Conv2D(filters=filters, kernel_size=[1, 1], strides=[1, 1], padding=\"same\")(out)\n",
        "  out = keras.layers.add([res,out])\n",
        "  return out\n",
        "\n",
        "def MiniModel(input_shape):\n",
        " images = Input(input_shape)\n",
        " net = Conv2D(filters=32, kernel_size=[3, 3], strides=[1, 1], padding=\"same\")(images)\n",
        " net = Unit(net,32)\n",
        " net = Unit(net,32)\n",
        " net = Unit(net,64,pool=True)\n",
        " net = Unit(net,64)\n",
        " net = Unit(net, 64)\n",
        " net = Unit(net,128,pool=True)\n",
        " net = Unit(net,128)\n",
        " net = Unit(net, 128)\n",
        " net = Unit(net, 256,pool=True)\n",
        " net = Unit(net, 256)\n",
        " net = Unit(net, 256)\n",
        " net = BatchNormalization()(net)\n",
        " net = Activation(\"relu\")(net)\n",
        " net = Dropout(0.25)(net)\n",
        " net = AveragePooling2D(pool_size=(4,4))(net)\n",
        " net = Flatten()(net)\n",
        " net = Dense(units=10,activation=\"softmax\")(net)\n",
        " model = Model(inputs=images,outputs=net)\n",
        " return model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}